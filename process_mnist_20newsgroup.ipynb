{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import sklearn.manifold\n",
    "import sklearn.datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import MulticoreTSNE\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print (twenty_train.data[1])\n",
    "print (twenty_train.target_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "vocab = count_vect.vocabulary_ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print (type(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "print (X_train_counts.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse.save_npz('data_sparse.npz', X_train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_matrix(m,filename, header=''):\n",
    "    thefile = open(filename, 'w')\n",
    "    thefile.write('FIRST_KEY, SECOND_KEY, OCCURENCES \\n')\n",
    "    nonZeros = np.array(m.nonzero()) \n",
    "    for entry in range(nonZeros.shape[1]):\n",
    "        thefile.write(\"%s,%s,%s\\n\" % (nonZeros[0, entry], nonZeros[1, entry], m[nonZeros[0, entry], nonZeros[1, entry]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sparse_matrix(X_train_counts, \"20groups_bag_of_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('20groups_word_mapping.csv', 'w') as f:  \n",
    "    w = csv.writer(f)\n",
    "    f.write('WORD,KEY \\n')\n",
    "    w.writerows(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing t-SNE embedding\n"
     ]
    }
   ],
   "source": [
    "digits = sklearn.datasets.load_digits()\n",
    "orig_X = digits.data\n",
    "orig_Y = digits.target\n",
    "\n",
    "every_nth_element = 1 #defult 1 == every element\n",
    "X = orig_X[0::every_nth_element]\n",
    "y = orig_Y[0::every_nth_element]\n",
    "\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = sklearn.manifold.TSNE(n_components=3, init='pca', random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "print(X_tsne)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (X_tsne.shape)\n",
    "print (y_reshaped.shape)\n",
    "y_reshaped = y.reshape(1797, 1)\n",
    "\n",
    "merged_X_Y = np.concatenate((X_tsne, y_reshaped), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.savetxt(\"mnist_small_tsne.csv\", merged_X_Y, fmt='%.5e',  delimiter=\",\", \n",
    "              header='X,Y,Z,label', comments='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_mnist_and_save(number_of_digits):\n",
    "    mnist = sklearn.datasets.fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_Y = shuffle(mnist.data, mnist.target)\n",
    "    full_mnist_data = mnist_X[:number_of_digits]\n",
    "    full_mnist_labels = mnist_Y[:number_of_digits]\n",
    "    pca = PCA(n_components=30)\n",
    "    full_mnist_after_pca = pca.fit_transform(full_mnist_data)\n",
    "    tsne = MulticoreTSNE.MulticoreTSNE(n_components=3, random_state=0, n_jobs=8)\n",
    "    full_mnist_after_tsne = tsne.fit_transform(full_mnist_after_pca)\n",
    "    full_mnist_labels = full_mnist_labels.reshape(number_of_digits, 1)\n",
    "    merged_full_mnist = np.concatenate((full_mnist_after_tsne, full_mnist_labels), axis=1)\n",
    "    file_name = \"mnist_\" + str(number_of_digits) + \".csv\"\n",
    "    np.savetxt(file_name, merged_full_mnist, fmt='%.5e',  delimiter=\",\", \n",
    "              header='X,Y,Z,label', comments='')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduce_mnist_and_save(70000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
